{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOG4T99YbiGVkf0yqMSh9MM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Huzzi-10/PiLENS-Real-Time-AI-Based-Suspicious-Activity-Detection-with-NightVision-on-Raspberry-Pi/blob/main/Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "#  A. Mount Drive & install\n",
        "# ----------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install required packages (ultralytics includes YOLO11 support)\n",
        "!pip install -q ultralytics==0.12.0  # if version issues, omit version to get latest\n",
        "!pip install -q torch torchvision opencv-python\n",
        "\n",
        "# ----------------------------\n",
        "#  B. Imports & Settings\n",
        "# ----------------------------\n",
        "import os, cv2, time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# ---- PATHS (change only if your Drive layout differs) ----\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive\"\n",
        "DATASET_DIR = os.path.join(DRIVE_ROOT, \"Dataset\")            # must contain Normal/ and Suspicious/\n",
        "FRAMES_DIR  = os.path.join(DRIVE_ROOT, \"Processed_Frames\")   # output frames (will be created)\n",
        "SAVE_MODEL_PATH = os.path.join(DRIVE_ROOT, \"action_model.pth\")\n",
        "\n",
        "# ---- CLASSES (must match folder names in Dataset/) ----\n",
        "CLASSES = [\"Normal\", \"Suspicious\"]\n",
        "\n",
        "# ---- YOLO11 small/nano model name ----\n",
        "YOLO_MODEL_NAME = \"yolo11n.pt\"   # using YOLOv11-nano as you wanted \"yolov11n\"\n",
        "\n",
        "# ---- device / hyperparams ----\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "SEQUENCE_LEN = 16     # number of frames per sample\n",
        "IMG_SIZE = 112\n",
        "BATCH_SIZE = 2        # low-ish to avoid OOM on Colab; increase if GPU memory allows\n",
        "EPOCHS = 6\n",
        "\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# ----------------------------\n",
        "#  C. Prepare transforms & load YOLO\n",
        "# ----------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "])\n",
        "\n",
        "# Download/Load YOLO11n model (ultralytics will fetch from the hub if not present)\n",
        "yolo = YOLO(YOLO_MODEL_NAME)\n",
        "\n",
        "# Quick check: ensure dataset folders exist\n",
        "for cls in CLASSES:\n",
        "    path = os.path.join(DATASET_DIR, cls)\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Expected dataset folder not found: {path}\")\n",
        "print(\"Dataset folders found:\", os.listdir(DATASET_DIR))\n",
        "\n",
        "# ----------------------------\n",
        "#  D. Extract person crops from videos -> save frames to Drive\n",
        "# ----------------------------\n",
        "os.makedirs(FRAMES_DIR, exist_ok=True)\n",
        "for cls in CLASSES:\n",
        "    out_cls_dir = os.path.join(FRAMES_DIR, cls)\n",
        "    os.makedirs(out_cls_dir, exist_ok=True)\n",
        "\n",
        "print(\"Starting frame extraction (this may take time depending on dataset size)...\")\n",
        "\n",
        "def extract_person_frames(video_path, out_folder, skip_every_n=1):\n",
        "    \"\"\"Extract crops for detected persons from one video and save JPGs to out_folder.\n",
        "       skip_every_n: save every nth frame to reduce frame count (1 -> every frame).\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    idx = 0\n",
        "    saved = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        # optional skipping to reduce size: if idx % skip_every_n != 0: idx += 1; continue\n",
        "        # Run YOLO on frame\n",
        "        try:\n",
        "            results = yolo(frame, verbose=False)\n",
        "        except Exception as e:\n",
        "            # fallback: skip problematic frames\n",
        "            idx += 1\n",
        "            continue\n",
        "        # results can be a list of Result objects; use first result\n",
        "        for r in results:\n",
        "            for box in r.boxes:\n",
        "                cls_id = int(box.cls[0])\n",
        "                if cls_id == 0:  # person\n",
        "                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                    # Safety clamp\n",
        "                    h, w = frame.shape[:2]\n",
        "                    x1, y1 = max(0, x1), max(0, y1)\n",
        "                    x2, y2 = min(w-1, x2), min(h-1, y2)\n",
        "                    crop = frame[y1:y2, x1:x2]\n",
        "                    if crop is None or crop.size == 0:\n",
        "                        continue\n",
        "                    fname = os.path.join(out_folder, f\"{os.path.basename(video_path).split('.')[0]}_{idx}.jpg\")\n",
        "                    cv2.imwrite(fname, crop)\n",
        "                    saved += 1\n",
        "        idx += 1\n",
        "    cap.release()\n",
        "    return saved\n",
        "\n",
        "# Walk dataset folders and extract frames\n",
        "total_saved = 0\n",
        "for cls in CLASSES:\n",
        "    in_folder = os.path.join(DATASET_DIR, cls)\n",
        "    out_folder = os.path.join(FRAMES_DIR, cls)\n",
        "    os.makedirs(out_folder, exist_ok=True)\n",
        "    files = sorted(os.listdir(in_folder))\n",
        "    for f in files:\n",
        "        if f.lower().endswith((\".mp4\", \".avi\", \".mov\", \".mkv\")):\n",
        "            vpath = os.path.join(in_folder, f)\n",
        "            print(\"Processing\", vpath)\n",
        "            saved = extract_person_frames(vpath, out_folder, skip_every_n=1)  # set skip_every_n=2 to save every other frame\n",
        "            print(f\"Saved {saved} frames for {f}\")\n",
        "            total_saved += saved\n",
        "\n",
        "print(\"Frame extraction finished. Total saved:\", total_saved)\n",
        "\n",
        "# ----------------------------\n",
        "#  E. Sequence Dataset: group frames into SEQUENCE_LEN sequences\n",
        "# ----------------------------\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, frames_root, seq_len=SEQUENCE_LEN, transform=transform):\n",
        "        self.samples = []\n",
        "        self.labels = []\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transform\n",
        "\n",
        "        for lbl, cname in enumerate(CLASSES):\n",
        "            folder = os.path.join(frames_root, cname)\n",
        "            if not os.path.exists(folder):\n",
        "                continue\n",
        "            files = sorted([f for f in os.listdir(folder) if f.lower().endswith((\".jpg\",\".png\",\".jpeg\"))])\n",
        "            paths = [os.path.join(folder, f) for f in files]\n",
        "            # make sequences in a sliding window or non-overlapping; here non-overlapping\n",
        "            for i in range(0, max(0, len(paths) - seq_len + 1), seq_len):\n",
        "                seq = paths[i:i+seq_len]\n",
        "                if len(seq) == seq_len:\n",
        "                    self.samples.append(seq)\n",
        "                    self.labels.append(lbl)\n",
        "\n",
        "        print(f\"Built dataset with {len(self.samples)} sequences.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq_paths = self.samples[idx]\n",
        "        frames = []\n",
        "        for p in seq_paths:\n",
        "            img = cv2.imread(p)\n",
        "            if img is None:\n",
        "                # pad with zeros if file missing\n",
        "                img = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = self.transform(img)\n",
        "            frames.append(img.unsqueeze(0))  # (1,3,H,W)\n",
        "        frames = torch.cat(frames, dim=0)  # (T,3,H,W)\n",
        "        # return as (T,C,H,W) â€” model expects (B,T,C,H,W) after batching\n",
        "        return frames, self.labels[idx]\n",
        "\n",
        "# Build dataset + dataloader\n",
        "dataset = SequenceDataset(FRAMES_DIR, seq_len=SEQUENCE_LEN)\n",
        "if len(dataset) == 0:\n",
        "    raise RuntimeError(\"No sequences were created. Check that frames were saved into Processed_Frames/<class>/\")\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "# ----------------------------\n",
        "#  F. Model: ResNet18 (feature extractor) + LSTM\n",
        "# ----------------------------\n",
        "class CNN_LSTM(nn.Module):\n",
        "    def __init__(self, feature_dim=512, hidden_dim=128, num_classes=len(CLASSES)):\n",
        "        super().__init__()\n",
        "        resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "        resnet.fc = nn.Identity()\n",
        "        self.cnn = resnet  # returns 512-d features per frame\n",
        "        self.lstm = nn.LSTM(feature_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, C, H, W)\n",
        "        B, T, C, H, W = x.shape\n",
        "        # flatten time & batch to feed through CNN\n",
        "        x = x.view(B*T, C, H, W)\n",
        "        feats = self.cnn(x)               # (B*T, 512)\n",
        "        feats = feats.view(B, T, -1)      # (B, T, 512)\n",
        "        lstm_out, _ = self.lstm(feats)    # (B, T, hidden_dim)\n",
        "        out = self.fc(lstm_out[:, -1, :]) # use last timestep\n",
        "        return out\n",
        "\n",
        "model = CNN_LSTM().to(DEVICE)\n",
        "print(\"Model created. Parameters:\", sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "# ----------------------------\n",
        "#  G. Training loop (simple)\n",
        "# ----------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    t0 = time.time()\n",
        "    for X, y in loader:\n",
        "        # X: (B, T, C, H, W) currently (B, T, C, H, W) due to collate defaults? Our __getitem__ returns (T,3,H,W)\n",
        "        # If loader gives X as (B, T, C, H, W) or (B, T, C, H, W) â€” ensure shape:\n",
        "        # We want (B, T, C, H, W)\n",
        "        if X.dim() == 4:\n",
        "            # case: (B*T,C,H,W) unlikely, but guard:\n",
        "            pass\n",
        "        # If X shape is (B, T, C, H, W) good; if (B, T, C, H, W) as expected.\n",
        "        X = X.to(DEVICE).float()\n",
        "        # ensure shape is (B, T, C, H, W)\n",
        "        if X.dim() == 4 and X.shape[1] == SEQUENCE_LEN:\n",
        "            # (B, T, H, W) incorrect layout; but our Dataset returns (T,C,H,W) and DataLoader stacks to (B,T,C,H,W)\n",
        "            pass\n",
        "        y = torch.tensor(y).to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X)   # (B, num_classes)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    dt = time.time() - t0\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}  Loss: {epoch_loss:.4f}  Time: {dt:.1f}s\")\n",
        "\n",
        "# ----------------------------\n",
        "#  H. Save trained model to Drive\n",
        "# ----------------------------\n",
        "torch.save(model.state_dict(), SAVE_MODEL_PATH)\n",
        "print(\"Model saved to:\", SAVE_MODEL_PATH)\n",
        "\n",
        "# ----------------------------\n",
        "#  I. Notes & Next Steps\n",
        "# ----------------------------\n",
        "print(\"DONE pipeline. You can now download action_model.pth and copy to your Raspberry Pi for inference.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8zqieRnmZmm",
        "outputId": "11e19375-a9df-4bad-8d69-167a0f308d8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\u001b[31mERROR: Ignored the following yanked versions: 8.0.129, 8.0.174, 8.0.177, 8.1.21, 8.1.31, 8.2.7, 8.2.47\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Ignored the following versions that require a different python version: 8.0.10 Requires-Python >=3.7,<=3.11; 8.0.11 Requires-Python >=3.7,<=3.11; 8.0.12 Requires-Python >=3.7,<=3.11; 8.0.13 Requires-Python >=3.7,<=3.11; 8.0.14 Requires-Python >=3.7,<=3.11; 8.0.15 Requires-Python >=3.7,<=3.11; 8.0.16 Requires-Python >=3.7,<=3.11; 8.0.17 Requires-Python >=3.7,<=3.11; 8.0.18 Requires-Python >=3.7,<=3.11; 8.0.19 Requires-Python >=3.7,<=3.11; 8.0.20 Requires-Python >=3.7,<=3.11; 8.0.21 Requires-Python >=3.7,<=3.11; 8.0.22 Requires-Python >=3.7,<=3.11; 8.0.23 Requires-Python >=3.7,<=3.11; 8.0.24 Requires-Python >=3.7,<=3.11; 8.0.25 Requires-Python >=3.7,<=3.11; 8.0.26 Requires-Python >=3.7,<=3.11; 8.0.27 Requires-Python >=3.7,<=3.11; 8.0.28 Requires-Python >=3.7,<=3.11; 8.0.29 Requires-Python >=3.7,<=3.11; 8.0.30 Requires-Python >=3.7,<=3.11; 8.0.31 Requires-Python >=3.7,<=3.11; 8.0.32 Requires-Python >=3.7,<=3.11; 8.0.33 Requires-Python >=3.7,<=3.11; 8.0.34 Requires-Python >=3.7,<=3.11; 8.0.7 Requires-Python >=3.7,<=3.11; 8.0.8 Requires-Python >=3.7,<=3.11; 8.0.9 Requires-Python >=3.7,<=3.11\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement ultralytics==0.12.0 (from versions: 0.0.13, 0.0.14, 0.0.15, 0.0.16, 0.0.17, 0.0.18, 0.0.19, 0.0.20, 0.0.21, 0.0.22, 0.0.23, 0.0.24, 0.0.25, 0.0.26, 0.0.27, 0.0.28, 0.0.29, 0.0.30, 0.0.31, 0.0.32, 0.0.33, 0.0.34, 0.0.35, 0.0.36, 0.0.37, 0.0.38, 0.0.39, 0.0.40, 0.0.41, 0.0.42, 0.0.43, 0.0.44, 8.0.0, 8.0.1, 8.0.2, 8.0.3, 8.0.4, 8.0.5, 8.0.6, 8.0.35, 8.0.36, 8.0.37, 8.0.38, 8.0.39, 8.0.40, 8.0.41, 8.0.42, 8.0.43, 8.0.44, 8.0.45, 8.0.46, 8.0.47, 8.0.48, 8.0.49, 8.0.50, 8.0.51, 8.0.52, 8.0.53, 8.0.54, 8.0.55, 8.0.56, 8.0.57, 8.0.58, 8.0.59, 8.0.60, 8.0.61, 8.0.62, 8.0.63, 8.0.64, 8.0.65, 8.0.66, 8.0.67, 8.0.68, 8.0.69, 8.0.70, 8.0.71, 8.0.72, 8.0.73, 8.0.74, 8.0.75, 8.0.76, 8.0.77, 8.0.78, 8.0.79, 8.0.80, 8.0.81, 8.0.82, 8.0.83, 8.0.84, 8.0.85, 8.0.86, 8.0.87, 8.0.88, 8.0.89, 8.0.90, 8.0.91, 8.0.92, 8.0.93, 8.0.94, 8.0.95, 8.0.96, 8.0.97, 8.0.98, 8.0.99, 8.0.100, 8.0.101, 8.0.102, 8.0.103, 8.0.104, 8.0.105, 8.0.106, 8.0.107, 8.0.108, 8.0.109, 8.0.110, 8.0.111, 8.0.112, 8.0.113, 8.0.114, 8.0.115, 8.0.116, 8.0.117, 8.0.118, 8.0.119, 8.0.120, 8.0.121, 8.0.122, 8.0.123, 8.0.124, 8.0.125, 8.0.126, 8.0.127, 8.0.128, 8.0.130, 8.0.131, 8.0.132, 8.0.133, 8.0.134, 8.0.135, 8.0.136, 8.0.137, 8.0.138, 8.0.139, 8.0.140, 8.0.141, 8.0.142, 8.0.143, 8.0.144, 8.0.145, 8.0.146, 8.0.147, 8.0.148, 8.0.149, 8.0.150, 8.0.151, 8.0.152, 8.0.153, 8.0.154, 8.0.155, 8.0.156, 8.0.157, 8.0.158, 8.0.159, 8.0.160, 8.0.161, 8.0.162, 8.0.163, 8.0.164, 8.0.165, 8.0.166, 8.0.167, 8.0.168, 8.0.169, 8.0.170, 8.0.171, 8.0.172, 8.0.173, 8.0.175, 8.0.176, 8.0.178, 8.0.179, 8.0.180, 8.0.181, 8.0.182, 8.0.183, 8.0.184, 8.0.185, 8.0.186, 8.0.187, 8.0.188, 8.0.189, 8.0.190, 8.0.191, 8.0.192, 8.0.193, 8.0.194, 8.0.195, 8.0.196, 8.0.197, 8.0.198, 8.0.199, 8.0.200, 8.0.201, 8.0.202, 8.0.203, 8.0.204, 8.0.205, 8.0.206, 8.0.207, 8.0.208, 8.0.209, 8.0.210, 8.0.211, 8.0.212, 8.0.213, 8.0.214, 8.0.215, 8.0.216, 8.0.217, 8.0.218, 8.0.219, 8.0.220, 8.0.221, 8.0.222, 8.0.223, 8.0.224, 8.0.225, 8.0.226, 8.0.227, 8.0.228, 8.0.229, 8.0.230, 8.0.231, 8.0.232, 8.0.233, 8.0.234, 8.0.235, 8.0.236, 8.0.237, 8.0.238, 8.0.239, 8.1.0, 8.1.1, 8.1.2, 8.1.3, 8.1.4, 8.1.5, 8.1.6, 8.1.7, 8.1.8, 8.1.9, 8.1.10, 8.1.11, 8.1.12, 8.1.13, 8.1.14, 8.1.15, 8.1.16, 8.1.17, 8.1.18, 8.1.19, 8.1.20, 8.1.22, 8.1.23, 8.1.24, 8.1.25, 8.1.26, 8.1.27, 8.1.28, 8.1.29, 8.1.30, 8.1.32, 8.1.33, 8.1.34, 8.1.35, 8.1.36, 8.1.37, 8.1.38, 8.1.39, 8.1.40, 8.1.41, 8.1.42, 8.1.43, 8.1.44, 8.1.45, 8.1.46, 8.1.47, 8.2.0, 8.2.1, 8.2.2, 8.2.3, 8.2.4, 8.2.5, 8.2.6, 8.2.8, 8.2.9, 8.2.10, 8.2.11, 8.2.12, 8.2.13, 8.2.14, 8.2.15, 8.2.16, 8.2.17, 8.2.18, 8.2.19, 8.2.20, 8.2.21, 8.2.22, 8.2.23, 8.2.24, 8.2.25, 8.2.26, 8.2.27, 8.2.28, 8.2.29, 8.2.30, 8.2.31, 8.2.32, 8.2.33, 8.2.34, 8.2.35, 8.2.36, 8.2.37, 8.2.38, 8.2.39, 8.2.40, 8.2.41, 8.2.42, 8.2.43, 8.2.44, 8.2.45, 8.2.46, 8.2.48, 8.2.49, 8.2.50, 8.2.51, 8.2.52, 8.2.53, 8.2.54, 8.2.55, 8.2.56, 8.2.57, 8.2.58, 8.2.59, 8.2.60, 8.2.61, 8.2.62, 8.2.63, 8.2.64, 8.2.65, 8.2.66, 8.2.67, 8.2.68, 8.2.69, 8.2.70, 8.2.71, 8.2.72, 8.2.73, 8.2.74, 8.2.75, 8.2.76, 8.2.77, 8.2.78, 8.2.79, 8.2.80, 8.2.81, 8.2.82, 8.2.83, 8.2.84, 8.2.85, 8.2.86, 8.2.87, 8.2.88, 8.2.89, 8.2.90, 8.2.91, 8.2.92, 8.2.93, 8.2.94, 8.2.95, 8.2.96, 8.2.97, 8.2.98, 8.2.99, 8.2.100, 8.2.101, 8.2.102, 8.2.103, 8.3.0, 8.3.1, 8.3.2, 8.3.3, 8.3.4, 8.3.5, 8.3.6, 8.3.7, 8.3.8, 8.3.9, 8.3.10, 8.3.11, 8.3.12, 8.3.13, 8.3.14, 8.3.15, 8.3.16, 8.3.17, 8.3.18, 8.3.19, 8.3.20, 8.3.21, 8.3.22, 8.3.23, 8.3.24, 8.3.25, 8.3.26, 8.3.27, 8.3.28, 8.3.29, 8.3.30, 8.3.31, 8.3.32, 8.3.33, 8.3.34, 8.3.35, 8.3.36, 8.3.37, 8.3.38, 8.3.39, 8.3.40, 8.3.43, 8.3.44, 8.3.47, 8.3.48, 8.3.49, 8.3.50, 8.3.51, 8.3.52, 8.3.53, 8.3.54, 8.3.55, 8.3.56, 8.3.57, 8.3.58, 8.3.59, 8.3.60, 8.3.61, 8.3.62, 8.3.63, 8.3.64, 8.3.65, 8.3.66, 8.3.67, 8.3.68, 8.3.69, 8.3.70, 8.3.71, 8.3.72, 8.3.73, 8.3.74, 8.3.75, 8.3.76, 8.3.77, 8.3.78, 8.3.79, 8.3.80, 8.3.81, 8.3.82, 8.3.83, 8.3.84, 8.3.85, 8.3.86, 8.3.87, 8.3.88, 8.3.89, 8.3.90, 8.3.91, 8.3.92, 8.3.93, 8.3.94, 8.3.95, 8.3.96, 8.3.97, 8.3.98, 8.3.99, 8.3.100, 8.3.101, 8.3.102, 8.3.103, 8.3.104, 8.3.105, 8.3.106, 8.3.107, 8.3.108, 8.3.109, 8.3.110, 8.3.111, 8.3.112, 8.3.113, 8.3.114, 8.3.115, 8.3.116, 8.3.117, 8.3.118, 8.3.119, 8.3.120, 8.3.121, 8.3.122, 8.3.123, 8.3.124, 8.3.125, 8.3.126, 8.3.127, 8.3.128, 8.3.129, 8.3.130, 8.3.131, 8.3.132, 8.3.133, 8.3.134, 8.3.135, 8.3.136, 8.3.137, 8.3.138, 8.3.139, 8.3.140, 8.3.141, 8.3.142, 8.3.143, 8.3.144, 8.3.145, 8.3.146, 8.3.147, 8.3.148, 8.3.149, 8.3.150, 8.3.151, 8.3.152, 8.3.153, 8.3.154, 8.3.155, 8.3.156, 8.3.157, 8.3.158, 8.3.159, 8.3.160, 8.3.161, 8.3.162, 8.3.163, 8.3.164, 8.3.165, 8.3.166, 8.3.167, 8.3.168, 8.3.169, 8.3.170, 8.3.171, 8.3.172, 8.3.173, 8.3.174, 8.3.175, 8.3.176, 8.3.177, 8.3.178, 8.3.179, 8.3.180, 8.3.181, 8.3.182, 8.3.183, 8.3.184, 8.3.185, 8.3.186, 8.3.187, 8.3.188, 8.3.189, 8.3.190, 8.3.191, 8.3.192, 8.3.193, 8.3.194, 8.3.195, 8.3.196, 8.3.197, 8.3.198, 8.3.199, 8.3.200, 8.3.201, 8.3.202, 8.3.203, 8.3.204, 8.3.205, 8.3.206, 8.3.207, 8.3.208, 8.3.209, 8.3.210, 8.3.211, 8.3.212, 8.3.213, 8.3.214, 8.3.215, 8.3.216, 8.3.217, 8.3.218, 8.3.219, 8.3.220, 8.3.221, 8.3.222, 8.3.223, 8.3.224, 8.3.225, 8.3.226, 8.3.227, 8.3.228, 8.3.229, 8.3.230, 8.3.231, 8.3.232, 8.3.233, 8.3.234)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for ultralytics==0.12.0\u001b[0m\u001b[31m\n",
            "\u001b[0mDevice: cuda\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.4MB 93.1MB/s 0.1s\n",
            "Dataset folders found: ['Normal', 'Suspicious']\n",
            "Starting frame extraction (this may take time depending on dataset size)...\n",
            "Processing /content/drive/MyDrive/Dataset/Normal/Main Gate PTZ Camera_20251103173948_20251103174316.mp4\n",
            "Saved 36679 frames for Main Gate PTZ Camera_20251103173948_20251103174316.mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Normal/Main Gate PTZ Camera_20251104070105_20251104070306.mp4\n",
            "Saved 31507 frames for Main Gate PTZ Camera_20251104070105_20251104070306.mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Normal/Running (1).mp4\n",
            "Saved 1123 frames for Running (1).mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Normal/Running (2).mp4\n",
            "Saved 5617 frames for Running (2).mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Normal/Running (3).mp4\n",
            "Saved 1547 frames for Running (3).mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Normal/Running (4).mp4\n",
            "Saved 865 frames for Running (4).mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Normal/Running (5).mp4\n",
            "Saved 1004 frames for Running (5).mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Normal/Running (6).mp4\n",
            "Saved 3701 frames for Running (6).mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Normal/Walking (1).mp4\n",
            "Saved 1110 frames for Walking (1).mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Normal/Walking (2).mp4\n",
            "Saved 2447 frames for Walking (2).mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Normal/WhatsApp Video 2025-12-02 at 18.20.33_8f3b68ec.mp4\n",
            "Saved 2418 frames for WhatsApp Video 2025-12-02 at 18.20.33_8f3b68ec.mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Suspicious/Fighting 1.mp4\n",
            "Saved 46775 frames for Fighting 1.mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Suspicious/Fighting 2.mp4\n",
            "Saved 8173 frames for Fighting 2.mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Suspicious/Fighting.mp4\n",
            "Saved 3628 frames for Fighting.mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Suspicious/Protest 1.mp4\n",
            "Saved 22071 frames for Protest 1.mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Suspicious/Protest 2.mp4\n",
            "Saved 2201 frames for Protest 2.mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Suspicious/Protest 3.mp4\n",
            "Saved 1544 frames for Protest 3.mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Suspicious/Protest 4.mp4\n",
            "Saved 1701 frames for Protest 4.mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Suspicious/Protest 5.mp4\n",
            "Saved 4267 frames for Protest 5.mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Suspicious/Smoking 1.mp4\n",
            "Saved 2263 frames for Smoking 1.mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Suspicious/Smoking 2.mp4\n",
            "Saved 307 frames for Smoking 2.mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Suspicious/Smoking 3.mp4\n",
            "Saved 3871 frames for Smoking 3.mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Suspicious/Smoking 4.mp4\n",
            "Saved 1050 frames for Smoking 4.mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Suspicious/Smoking 5.mp4\n",
            "Saved 676 frames for Smoking 5.mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Suspicious/Smoking 6.mp4\n",
            "Saved 2337 frames for Smoking 6.mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Suspicious/WhatsApp Video 2025-12-02 at 18.20.27_cd09d6fb.mp4\n",
            "Saved 4024 frames for WhatsApp Video 2025-12-02 at 18.20.27_cd09d6fb.mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Suspicious/WhatsApp Video 2025-12-02 at 18.20.28_015c6558 (online-video-cutter.com).mp4\n",
            "Saved 5648 frames for WhatsApp Video 2025-12-02 at 18.20.28_015c6558 (online-video-cutter.com).mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Suspicious/WhatsApp Video 2025-12-02 at 18.20.29_832579af (online-video-cutter.com).mp4\n",
            "Saved 3857 frames for WhatsApp Video 2025-12-02 at 18.20.29_832579af (online-video-cutter.com).mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Suspicious/WhatsApp Video 2025-12-02 at 18.20.30_dfc9c49c.mp4\n",
            "Saved 8173 frames for WhatsApp Video 2025-12-02 at 18.20.30_dfc9c49c.mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Suspicious/WhatsApp Video 2025-12-02 at 18.20.31_cf20991f.mp4\n",
            "Saved 9891 frames for WhatsApp Video 2025-12-02 at 18.20.31_cf20991f.mp4\n",
            "Processing /content/drive/MyDrive/Dataset/Suspicious/WhatsApp Video 2025-12-02 at 18.20.32_5f557695 (online-video-cutter.com).mp4\n",
            "Saved 9685 frames for WhatsApp Video 2025-12-02 at 18.20.32_5f557695 (online-video-cutter.com).mp4\n",
            "Frame extraction finished. Total saved: 230160\n",
            "Built dataset with 1679 sequences.\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 134MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model created. Parameters: 11505474\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1371033146.py:225: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y = torch.tensor(y).to(DEVICE)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6  Loss: 281.9593  Time: 263.2s\n",
            "Epoch 2/6  Loss: 91.9694  Time: 166.9s\n",
            "Epoch 3/6  Loss: 60.3379  Time: 162.1s\n",
            "Epoch 4/6  Loss: 55.3160  Time: 157.6s\n",
            "Epoch 5/6  Loss: 27.5835  Time: 156.8s\n",
            "Epoch 6/6  Loss: 48.7931  Time: 155.9s\n",
            "Model saved to: /content/drive/MyDrive/action_model.pth\n",
            "DONE pipeline. You can now download action_model.pth and copy to your Raspberry Pi for inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "#   A. Connect Google Drive\n",
        "# ============================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ============================================================\n",
        "#   B. Install required libraries\n",
        "# ============================================================\n",
        "!pip install -q torch torchvision opencv-python\n",
        "\n",
        "# ============================================================\n",
        "#   C. Imports\n",
        "# ============================================================\n",
        "import os, cv2, time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ============================================================\n",
        "#   D. Paths & Settings (CHANGE ONLY THESE 3 IF NEEDED)\n",
        "# ============================================================\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive\"\n",
        "FRAMES_DIR  = os.path.join(DRIVE_ROOT, \"Processed_Frames\")   # already created by your pipeline\n",
        "SAVE_MODEL_PATH = os.path.join(DRIVE_ROOT, \"action_model.pth\")\n",
        "\n",
        "CLASSES = [\"Normal\", \"Suspicious\"]\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "SEQUENCE_LEN = 16\n",
        "IMG_SIZE = 112\n",
        "BATCH_SIZE = 4           # you can increase to 8 if GPU supports\n",
        "EPOCHS = 8              # <-- tum jitne chaaho yahan change kar do\n",
        "\n",
        "print(\"Device:\", DEVICE)\n",
        "print(\"Frames directory:\", FRAMES_DIR)\n",
        "\n",
        "# ============================================================\n",
        "#   E. Transform settings\n",
        "# ============================================================\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "])\n",
        "\n",
        "# ============================================================\n",
        "#   F. Dataset Class (Sequence Dataset)\n",
        "# ============================================================\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, frames_root, seq_len=16, transform=None):\n",
        "        self.samples = []\n",
        "        self.labels = []\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transform\n",
        "\n",
        "        for lbl, cname in enumerate(CLASSES):\n",
        "            folder = os.path.join(frames_root, cname)\n",
        "            if not os.path.exists(folder):\n",
        "                print(\"Missing:\", folder)\n",
        "                continue\n",
        "\n",
        "            files = sorted([f for f in os.listdir(folder)\n",
        "                            if f.lower().endswith((\".jpg\", \".png\", \".jpeg\"))])\n",
        "\n",
        "            paths = [os.path.join(folder, f) for f in files]\n",
        "\n",
        "            # Non-overlapping sequence windows\n",
        "            for i in range(0, max(0, len(paths) - seq_len + 1), seq_len):\n",
        "                seq = paths[i:i+seq_len]\n",
        "                if len(seq) == seq_len:\n",
        "                    self.samples.append(seq)\n",
        "                    self.labels.append(lbl)\n",
        "\n",
        "        print(f\"Total sequences created: {len(self.samples)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq_paths = self.samples[idx]\n",
        "        frames = []\n",
        "\n",
        "        for p in seq_paths:\n",
        "            img = cv2.imread(p)\n",
        "            if img is None:\n",
        "                img = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\n",
        "\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = self.transform(img)\n",
        "            frames.append(img.unsqueeze(0))  # (1,3,H,W)\n",
        "\n",
        "        frames = torch.cat(frames, dim=0)  # (T,3,H,W)\n",
        "        return frames, self.labels[idx]\n",
        "\n",
        "# Dataset + DataLoader\n",
        "dataset = SequenceDataset(FRAMES_DIR, seq_len=SEQUENCE_LEN, transform=transform)\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "# ============================================================\n",
        "#   G. Model Definition (ResNet18 + LSTM)\n",
        "# ============================================================\n",
        "class CNN_LSTM(nn.Module):\n",
        "    def __init__(self, feature_dim=512, hidden_dim=128, num_classes=len(CLASSES)):\n",
        "        super().__init__()\n",
        "        resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "        resnet.fc = nn.Identity()    # remove final layer\n",
        "        self.cnn = resnet\n",
        "\n",
        "        self.lstm = nn.LSTM(feature_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B,T,C,H,W)\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.reshape(B*T, C, H, W)\n",
        "        feats = self.cnn(x)               # (B*T,512)\n",
        "        feats = feats.reshape(B, T, -1)   # (B,T,512)\n",
        "        lstm_out, _ = self.lstm(feats)\n",
        "        out = self.fc(lstm_out[:, -1, :]) # last frame output\n",
        "        return out\n",
        "\n",
        "model = CNN_LSTM().to(DEVICE)\n",
        "\n",
        "print(\"Model parameters:\", sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "# ============================================================\n",
        "#   H. Training Setup\n",
        "# ============================================================\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# ============================================================\n",
        "#   I. Training Loop\n",
        "# ============================================================\n",
        "print(\"\\nðŸ’¥ Starting Training...\\n\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    t0 = time.time()\n",
        "\n",
        "    for X, y in loader:\n",
        "        X = X.to(DEVICE).float()\n",
        "        y = torch.tensor(y).to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(X)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {total_loss:.4f} | Time: {time.time() - t0:.1f}s\")\n",
        "\n",
        "# ============================================================\n",
        "#   J. Save Model\n",
        "# ============================================================\n",
        "torch.save(model.state_dict(), SAVE_MODEL_PATH)\n",
        "print(\"\\nðŸŽ‰ Model Saved Successfully at:\", SAVE_MODEL_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0rfRti03TkL",
        "outputId": "7130c393-6dbd-4f24-92a0-4bc8ec90f440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Device: cuda\n",
            "Frames directory: /content/drive/MyDrive/Processed_Frames\n",
            "Total sequences created: 1679\n",
            "Model parameters: 11505474\n",
            "\n",
            "ðŸ’¥ Starting Training...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3790665116.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y = torch.tensor(y).to(DEVICE)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8 | Loss: 107.2894 | Time: 262.1s\n",
            "Epoch 2/8 | Loss: 50.9269 | Time: 154.4s\n",
            "Epoch 3/8 | Loss: 25.6259 | Time: 154.0s\n",
            "Epoch 4/8 | Loss: 16.0332 | Time: 153.4s\n",
            "Epoch 5/8 | Loss: 19.0876 | Time: 153.4s\n",
            "Epoch 6/8 | Loss: 19.8068 | Time: 153.6s\n",
            "Epoch 7/8 | Loss: 9.0061 | Time: 152.7s\n",
            "Epoch 8/8 | Loss: 8.2380 | Time: 153.4s\n",
            "\n",
            "ðŸŽ‰ Model Saved Successfully at: /content/drive/MyDrive/action_model.pth\n"
          ]
        }
      ]
    }
  ]
}